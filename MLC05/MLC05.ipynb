{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\", \"4042\", \"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawData = sc.textFile(\"train_noheader.tsv\")\n",
    "val records = rawData.map(line => line.split(\"\\t\"))\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "val data = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val features = trimmed.slice(4, r.size - 1).map(d => if (d ==\"?\") 0.0 else d.toDouble)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7395"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache\n",
    "val numData = data.count\n",
    "numData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nbData = records.map { r =>\n",
    "val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "val label = trimmed(r.size - 1).toInt\n",
    "val features = trimmed.slice(4, r.size - 1).map(d => \n",
    "    if (d ==\"?\") 0.0 else d.toDouble).map(d => \n",
    "        if (d < 0) 0.0 else d)\n",
    "        LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n",
    "import org.apache.spark.mllib.classification.SVMWithSGD\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "val numIterations = 10\n",
    "val maxTreeDepth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/* 逻辑回归 */\n",
    "val lrModel = LogisticRegressionWithSGD.train(data, numIterations)\n",
    "/* SVM 模型 */\n",
    "val svmModel = SVMWithSGD.train(data, numIterations)\n",
    "/* 朴素贝叶斯 */\n",
    "val nbModel = NaiveBayes.train(nbData)\n",
    "/* 决策树 */\n",
    "val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataPoint = data.first\n",
    "val prediction = lrModel.predict(dataPoint.features)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPoint.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.0, 1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = lrModel.predict(data.map(lp => lp.features))\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5146720757268425"
     ]
    }
   ],
   "source": [
    "val lrTotalCorrect = data.map { point =>\n",
    "if (lrModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracy = lrTotalCorrect / data.count\n",
    "print(lrAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3806.0"
     ]
    }
   ],
   "source": [
    "val svmTotalCorrect = data.map { point =>\n",
    "if (svmModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "print(svmTotalCorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4292.0"
     ]
    }
   ],
   "source": [
    "val nbTotalCorrect = nbData.map { point =>\n",
    "if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "print(nbTotalCorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4794.0"
     ]
    }
   ],
   "source": [
    "val dtTotalCorrect = data.map { point =>\n",
    "val score = dtModel.predict(point.features)\n",
    "val predicted = if (score > 0.5) 1 else 0\n",
    "if (predicted == point.label) 1 else 0\n",
    "}.sum\n",
    "print(dtTotalCorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5146720757268425"
     ]
    }
   ],
   "source": [
    "val svmAccuracy = svmTotalCorrect / numData\n",
    "print(svmAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5803921568627451"
     ]
    }
   ],
   "source": [
    "val nbAccuracy = nbTotalCorrect / numData\n",
    "print(nbAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6482758620689655"
     ]
    }
   ],
   "source": [
    "val dtAccuracy = dtTotalCorrect / numData\n",
    "print(dtAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import\n",
    "org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "val metrics = Seq(lrModel, svmModel).map { model =>\n",
    "val scoreAndLabels = data.map { point =>\n",
    "(model.predict(point.features), point.label)\n",
    "}\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "(model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val nbMetrics = Seq(nbModel).map{ model =>\n",
    "val scoreAndLabels = nbData.map { point =>\n",
    "val score = model.predict(point.features)\n",
    "(if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "}\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "(model.getClass.getSimpleName, metrics.areaUnderPR,\n",
    "metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%\n",
      "DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%\n"
     ]
    }
   ],
   "source": [
    "val dtMetrics = Seq(dtModel).map{ model =>\n",
    "val scoreAndLabels = data.map { point =>\n",
    "val score = model.predict(point.features)\n",
    "(if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "}\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "(model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
    "allMetrics.foreach{ case (m, pr, roc) =>\n",
    "println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6\n",
    "改进模型性能以及参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6.1 特征标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4122580529952672,2.761823191986608,0.4682304732861389,0.21407992638350232,0.09206236071899916,0.04926216043908053,2.255103452212041,-0.10375042752143335,0.0,0.05642274498417851,0.02123056118999324,0.23377817665490194,0.2757090373659236,0.615551048005409,0.6603110209601082,30.07707910750513,0.03975659229208925,5716.598242055447,178.75456389452353,4.960649087221096,0.17286405047031742,0.10122079189276552]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "val vectors = data.map(lp => lp.features)\n",
    "val matrix = new RowMatrix(vectors)\n",
    "val matrixSummary = matrix.computeColumnSummaryStatistics()\n",
    "println(matrixSummary.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]\n",
      "[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444,1.0,0.716883117,113.3333333,1.0,1.0,100.0,1.0,207952.0,4997.0,22.0,1.0,1.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.min)\n",
    "println(matrixSummary.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10974244167559001,74.30082476809639,0.04126316989120241,0.02153343633200108,0.009211817450882448,0.005274933469767946,32.53918714591821,0.09396988697611545,0.0,0.0017177410346628928,0.020782634824610638,0.0027548394224293036,3.683788919674426,0.2366799607085986,0.22433071201674218,415.8785589543846,0.03818116876739597,7.877330081138463E7,32208.116247426184,10.45300904576431,0.03359363403832393,0.006277532884214705]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0,7362.0,157.0,7395.0,7355.0,4552.0,4883.0,7347.0,294.0,7378.0,7395.0,6782.0,6868.0,7235.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.numNonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* 标准化数据 */\n",
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n",
    "val scaledData = data.map(lp => LabeledPoint(lp.label,\n",
    "scaler.transform(lp.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n"
     ]
    }
   ],
   "source": [
    "println(data.first.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "println(scaledData.first.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy:62.0419%\n",
      "Area under PR: 72.7254%\n",
      "Area under ROC: 61.9663%\n"
     ]
    }
   ],
   "source": [
    "val lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\n",
    "val lrTotalCorrectScaled = scaledData.map { point =>\n",
    "if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracyScaled = lrTotalCorrectScaled / numData\n",
    "val lrPredictionsVsTrue = scaledData.map { point =>\n",
    "(lrModelScaled.predict(point.features), point.label)\n",
    "}\n",
    "val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\n",
    "val lrPr = lrMetricsScaled.areaUnderPR\n",
    "val lrRoc = lrMetricsScaled.areaUnderROC\n",
    "println(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr *100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(\"weather\" -> 0, \"sports\" -> 6, \"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8, \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \"business\" -> 1, \"science_technology\" -> 7)\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "/* 1-of-k 编码 */\n",
    "val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
    "val numCategories = categories.size\n",
    "println(categories)\n",
    "println(numCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])\n"
     ]
    }
   ],
   "source": [
    "val dataCategories = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    val otherFeatures = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
    "    val features = categoryFeatures ++ otherFeatures\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "println(dataCategories.first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val scalerCats = new StandardScaler(withMean = true, withStd = true).\n",
    "fit(dataCategories.map(lp => lp.features))\n",
    "val scaledDataCats = dataCategories.map(lp =>\n",
    "    LabeledPoint(lp.label, scalerCats.transform(lp.features))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n"
     ]
    }
   ],
   "source": [
    "/* 标准化之前的特征 */\n",
    "println(dataCategories.first.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02326210589837061,2.7207366564548514,-0.4464212047941535,-0.22052688457880879,-0.028494000387023734,-0.2709990696925828,-0.23272797709480803,-0.2016540523193296,-0.09914991930875496,-0.38181322324318134,-0.06487757239262681,-0.6807527904251456,-0.20418221057887365,-0.10189469097220732,1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "/* 标准化之后的特征 */\n",
    "println(scaledDataCats.first.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy:66.5720%\n",
      "Area under PR: 75.7964%\n",
      "Area under ROC: 66.5483%\n"
     ]
    }
   ],
   "source": [
    "val lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats,\n",
    "numIterations)\n",
    "val lrTotalCorrectScaledCats = scaledDataCats.map { point =>\n",
    "if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\n",
    "val lrPredictionsVsTrueCats = scaledDataCats.map { point =>\n",
    "(lrModelScaledCats.predict(point.features), point.label)\n",
    "}\n",
    "val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\n",
    "val lrPrCats = lrMetricsScaledCats.areaUnderPR\n",
    "val lrRocCats = lrMetricsScaledCats.areaUnderROC\n",
    "println(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats *100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* 1-of-k 编码的类型特征更符合朴素贝叶斯模型 */\n",
    "val dataNB = records.map { r =>\n",
    "val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "val label = trimmed(r.size - 1).toInt\n",
    "val categoryIdx = categories(r(3))\n",
    "val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "categoryFeatures(categoryIdx) = 1.0\n",
    "LabeledPoint(label, Vectors.dense(categoryFeatures))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel\n",
      "Accuracy:60.9601%\n",
      "Area under PR: 74.0522%\n",
      "Area under ROC: 60.5138%\n"
     ]
    }
   ],
   "source": [
    "val nbModelCats = NaiveBayes.train(dataNB)\n",
    "val nbTotalCorrectCats = dataNB.map { point =>\n",
    "if (nbModelCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val nbAccuracyCats = nbTotalCorrectCats / numData\n",
    "val nbPredictionsVsTrueCats = dataNB.map { point =>\n",
    "(nbModelCats.predict(point.features), point.label)\n",
    "}\n",
    "val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\n",
    "val nbPrCats = nbMetricsCats.areaUnderPR\n",
    "val nbRocCats = nbMetricsCats.areaUnderROC\n",
    "println(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy:${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats *100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/* 1. 线性模型 */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/* 1. 线性模型 */\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.optimization.Updater\n",
    "import org.apache.spark.mllib.optimization.SimpleUpdater\n",
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
    "import org.apache.spark.mllib.classification.ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainWithParams(input: RDD[LabeledPoint], regParam: Double,\n",
    "numIterations: Int, updater: Updater, stepSize: Double) = {\n",
    "    val lr = new LogisticRegressionWithSGD\n",
    "        lr.optimizer.setNumIterations(numIterations).\n",
    "        setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
    "        lr.run(input)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createMetrics(label: String, data: RDD[LabeledPoint], model:ClassificationModel) = {\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        (model.predict(point.features),point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (label, metrics.areaUnderROC)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[502] at map at <console>:50"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaledDataCats.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations, AUC =64.95%\n",
      "5 iterations, AUC =66.62%\n",
      "10 iterations, AUC =66.55%\n",
      "50 iterations, AUC =66.81%\n"
     ]
    }
   ],
   "source": [
    "/* 迭代 */\n",
    "val iterResults = Seq(1, 5, 10, 50).map { param =>\n",
    "val model = trainWithParams(scaledDataCats, 0.0, param, new\n",
    "SimpleUpdater, 1.0)\n",
    "createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
    "}\n",
    "iterResults.foreach { case (param, auc) => println(f\"$param, AUC =${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 step size, AUC = 64.97%\n",
      "0.01 step size, AUC = 64.96%\n",
      "0.1 step size, AUC = 65.52%\n",
      "1.0 step size, AUC = 66.55%\n",
      "10.0 step size, AUC = 61.92%\n"
     ]
    }
   ],
   "source": [
    "/* 步长 */\n",
    "val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n",
    "createMetrics(s\"$param step size\", scaledDataCats, model)\n",
    "}\n",
    "stepResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 L2 regularization parameter, AUC =66.55%\n",
      "0.01 L2 regularization parameter, AUC =66.55%\n",
      "0.1 L2 regularization parameter, AUC =66.63%\n",
      "1.0 L2 regularization parameter, AUC =66.04%\n",
      "10.0 L2 regularization parameter, AUC =35.33%\n"
     ]
    }
   ],
   "source": [
    "/* 正则化 */\n",
    "val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "val model = trainWithParams(scaledDataCats, param, numIterations,\n",
    "new SquaredL2Updater, 1.0)\n",
    "createMetrics(s\"$param L2 regularization parameter\",\n",
    "scaledDataCats, model)\n",
    "}\n",
    "regResults.foreach { case (param, auc) => println(f\"$param, AUC =${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Gini\n",
    "def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\n",
    "    DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.88%\n",
      "10 tree depth, AUC = 76.26%\n",
      "20 tree depth, AUC = 98.45%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
    "    val model = trainDTWithParams(data, param, Entropy)\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "        }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsEntropy.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Gini\n",
    "def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int,\n",
    "impurity: Impurity) = {\n",
    "DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth,AUC = 59.33%\n",
      "2 tree depth,AUC = 61.68%\n",
      "3 tree depth,AUC = 62.61%\n",
      "4 tree depth,AUC = 63.63%\n",
      "5 tree depth,AUC = 64.88%\n",
      "10 tree depth,AUC = 76.26%\n",
      "20 tree depth,AUC = 98.45%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
    "    val model = trainDTWithParams(data, param, Entropy)\n",
    "    val scoreAndLabels = data.map { point =>\n",
    "    val score = model.predict(point.features)\n",
    "    (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsEntropy.foreach { case (param, auc) => println(f\"$param,AUC = ${auc * 100}%2.2f%%\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n",
    "    val nb = new NaiveBayes\n",
    "    nb.setLambda(lambda)\n",
    "    nb.run(input)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 lambda, AUC = 60.51%\n",
      "0.01 lambda, AUC = 60.51%\n",
      "0.1 lambda, AUC = 60.51%\n",
      "1.0 lambda, AUC = 60.51%\n",
      "10.0 lambda, AUC = 60.51%\n"
     ]
    }
   ],
   "source": [
    "val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainNBWithParams(dataNB, param)\n",
    "    val scoreAndLabels = dataNB.map { point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param lambda\", metrics.areaUnderROC)\n",
    "}\n",
    "nbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.1 (Scala 2.10.4)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
